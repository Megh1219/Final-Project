{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yi-c4WoUxRj"
      },
      "source": [
        "**Importing Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P-WcMEaoK2bg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Doh8BT2JpkOR"
      },
      "source": [
        "**Downloading dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuH43UBTU4Ce"
      },
      "outputs": [],
      "source": [
        "#Loading the dataset\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file(\"flower_photos\", origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "#Splitting the dataset into train and test images\n",
        "(train_images, train_lables), (evaluating_images, evaluating_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images_reshaped = train_images.reshape(60000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1NQXYapePi"
      },
      "source": [
        "**Functions for the task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mDD7KK9IhwTO"
      },
      "outputs": [],
      "source": [
        "#Function the inform user about the task\n",
        "def task_information():\n",
        "  print(\"Greetings of the day..!!!\\\n",
        "  This task usualyy focuses around the training of various models and saving all the necessary details of all the models\\\n",
        "  into csv files. The quantity of total number of models depends on user's choice and users are allowed to tune hyperparameter\\\n",
        "  at every layer of the individual model.  \")\n",
        "\n",
        "#Creating the model\n",
        "def build_model(num_layers, model_number, input_shape=(28, 28, 1)):\n",
        "  \n",
        "  #Necessary parameters for building the model\n",
        "  i = 1\n",
        "  filters = []\n",
        "  kernel_sizes = []\n",
        "  activations = []\n",
        "  pooling_sizes = []\n",
        "  paddings = []\n",
        "\n",
        "  epochs = int(input(\"Enter the number of epochs for Model {}: \".format(model_number)))\n",
        "  print(\"\")\n",
        "\n",
        "  #Loop to build the number of layers of the model, #Get idea from my cousin\n",
        "  while i < num_layers+1:\n",
        "    filter_value = int(input(\"Enter filter size for {} layer: \".format(i)))\n",
        "    filters.append(filter_value)\n",
        "\n",
        "    kernel_size_value = int(input(\"Enter kernel size for {} layer: \".format(i)))\n",
        "    kernel_sizes.append(kernel_size_value)\n",
        "\n",
        "    activation_value = input(\"Enter activation function for {} layer: \".format(i))\n",
        "    activations.append(activation_value)\n",
        "\n",
        "    pooling_size_value = int(input(\"Enter pooling size for {} layer: \".format(i)))\n",
        "    pooling_sizes.append(pooling_size_value)\n",
        "\n",
        "    padding_value = input(\"Enter the padding value for {} layer: \".format(i))\n",
        "    paddings.append(padding_value)\n",
        "\n",
        "    print()\n",
        "    i += 1\n",
        "\n",
        "  #Using sequential as base model for the neural network\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "  #Using zip function to implement crucial parameters in the layers, #Get idea from my cousin\n",
        "  for filter, kernel_size, activation, pool_size, padding, i in zip(filters, kernel_sizes, activations, pooling_sizes, paddings, range(1, num_layers+1)):\n",
        "      model.add(Conv2D(filters=filter, kernel_size=(kernel_size,kernel_size), activation=activation, padding=padding))\n",
        "      model.add(MaxPooling2D(pool_size=(pool_size,pool_size)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(units=10, activation='softmax'))\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  history = model.fit(train_images_reshaped, train_lables, epochs = epochs, validation_split = 0.4)\n",
        "  return history, model, epochs\n",
        "\n",
        "#Accumulating all the functions in one main function\n",
        "def main():\n",
        "  #Introduction of the task\n",
        "  task_information()\n",
        "  print(\"\")\n",
        "\n",
        "  number_models = int(input(\"Please enter the number of models: \"))\n",
        "  print(\"\")\n",
        "\n",
        "  dataframe = pd.DataFrame(columns=['Model Name', 'Architecture' ,'Training Accuracy', 'Validation Accuracy', 'Training Loss', 'Validation Loss', 'Epochs'])\n",
        "\n",
        "  i = 1\n",
        "  #Loop used to ask user to put parameters for each layers\n",
        "  while i < number_models+1:\n",
        "    summary = []\n",
        "    print(\"\")\n",
        "    print(\"Enter the details of achitecture and hyperparameters for Model {}\".format(i))\n",
        "    layers = int(input(\"Enter the layers for Model {}: \".format(i)))\n",
        "    print(\"\")\n",
        "    history, model, epochs = build_model(layers, i) #Calling build model to return hyperparameters and history of model\n",
        "    model.summary(print_fn=lambda x: summary.append(x)) #Using lambda function that will take number of arguments and returns only one expression #Get help from my cousin\n",
        "    dataframe.loc[i] = [model.name, summary[2:], max(history.history['accuracy']), max(history.history['val_accuracy']), min(history.history['loss']), min(history.history['val_loss']), epochs]\n",
        "    i += 1\n",
        "\n",
        "  #Saving the dataframe of information of all the models\n",
        "  print(dataframe)\n",
        "  dataframe.to_csv('output.csv', encoding = 'utf-8-sig', index=False) \n",
        "  files.download('output.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rg9G08uYust"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}